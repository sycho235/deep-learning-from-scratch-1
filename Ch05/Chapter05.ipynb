{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 05 오차역전파법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__수치 미분__\n",
    "- 단순하고 쉽다\n",
    "- 계산 시간이 오래 걸린다\n",
    "\n",
    "$\\rightarrow$ __오차역전파법(backpropagation)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 계산 그래프\n",
    "P.148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__게산 그래프__ : 계산 과정을 그래프로 나타낸 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 계산 그래프로 풀다\n",
    "\n",
    "<img src='./img/fig 5-1.png' width=500>\n",
    "\n",
    "<img src='./img/fig 5-2.png' width=500>\n",
    "\n",
    "<img src='./img/fig 5-3.png' width=500>\n",
    "\n",
    "__순전파__ : 계산을 왼쪽에서 오른쪽으로 진행\n",
    "\n",
    "__역전파__ : 계산을 오른쪽에서 왼쪽으로 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 국소적 계산\n",
    "\n",
    "국소적 계산 : 자신과 직접 관련된 작은 범위\n",
    ">전체와 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다.\n",
    "\n",
    "<img src='./img/fig 5-4.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 왜 계산 그래프로 푸는가?\n",
    "\n",
    "1. 국소적 계산\n",
    "2. 중간 계산 결과를 모두 보관\n",
    "3. 역전파를 통해 __미분__을 효과적으로 계산\n",
    "\n",
    "<img src='./img/fig 5-5.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 연쇄법칙\n",
    "P.152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 계산 그래프의 역전파\n",
    "\n",
    "신호 E에 노드의 국소적 미분($\\frac{\\partial y}{\\partial x}$)을 곱한 후 다음 노드로 전달\n",
    "\n",
    "<img src='./img/fig 5-6.png' width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 연쇄법칙이란?\n",
    "\n",
    "_**합성 함수**의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다._\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 연쇄법칙과 계산 그래프\n",
    "\n",
    "<img src='./img/fig 5-7.png' width=400>\n",
    "\n",
    "노드로 들어온 입력 신호에 그 노드의 국소적 미분(편미분)을 곱한 후 다음 노드로 전달\n",
    "\n",
    "\n",
    "<img src='./img/fig 5-8.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 역전파\n",
    "P.155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 덧셈 노드의 역전파\n",
    "\n",
    "$$z = x + y$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = 1$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = 1$$\n",
    "\n",
    ">덧셈 노드의 역전파는 입력된 값을 그대로 다음 노드로 보냄\n",
    ">\n",
    ">순방향 입력 신호의 값이 필요 없음\n",
    "\n",
    "<img src='./img/fig 5-9.png' width=500>\n",
    "\n",
    "<img src='./img/fig 5-10.png' width=500>\n",
    "\n",
    "\n",
    "예시)\n",
    "<img src='./img/fig 5-11.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 곱셈 노드의 역전파\n",
    "\n",
    "$$z = xy$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = y$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = x$$\n",
    "\n",
    ">곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보냄\n",
    ">\n",
    ">순방향 입력 신호의 값이 필요함 $\\Rightarrow$ 순전파의 입력 신호를 변수에 저장\n",
    "\n",
    "<img src='./img/fig 5-12.png' width=500>\n",
    "\n",
    "\n",
    "예시)\n",
    "<img src='./img/fig 5-13.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 사과 쇼핑의 예\n",
    "\n",
    "<img src='./img/fig 5-14.png' width=500>\n",
    "\n",
    "<img src='./img/fig 5-17.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층 구현하기\n",
    "P.160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 구성하는 '계층' 각각을 하나의 클래스로 구현\n",
    "\n",
    "__계층__ : 신경망의 기능 단위.  e.g. Sigmoid, Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y  # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/fig 5-16.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 덧셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/fig 5-17.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기\n",
    "P.165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU 계층\n",
    "\n",
    "$$y = \n",
    "\\begin{cases}\n",
    "x \\quad (x > 0)\\\\\n",
    "0 \\quad (x \\leq 0)\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = \n",
    "\\begin{cases}\n",
    "1 \\quad (x > 0)\\\\\n",
    "0 \\quad (x \\leq 0)\n",
    "\\end{cases}$$\n",
    "\n",
    "- 순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그대로 하류로 보냄.\n",
    "- 순전파 때의 입력인 x가 0 이하면 역전파는 하류로 신호를 보내지 않음.\n",
    "\n",
    "\n",
    "<img src='./img/fig 5-18.png' width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)  # mask는 True/False로 구성된 넘파이 배열\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0  # x의 원소 값이 0 이하인 원소는 0으로, 나머지는 그대로\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0  # x의 원소의 값이 0 이하면 역전파 때의 값이 0, 나머지는 그대로\n",
    "        dx = dout\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\n",
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층\n",
    "\n",
    "$$y = \\frac{1}{1 + exp(-x)}$$\n",
    "\n",
    "<img src='./img/fig 5-19.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 단계**\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = - \\frac{1}{x^2} = - y^2$$\n",
    "\n",
    "<img src='./img/fig 5-19(1).png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 단계**\n",
    "<img src='./img/fig 5-19(2).png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 단계**\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = exp(x)$$\n",
    "\n",
    "<img src='./img/fig 5-19(3).png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 단계**\n",
    "<img src='./img/fig 5-20.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**간소화 버전**\n",
    "\n",
    "- 순전파의 입력 x와 출력 y만으로 계산할 수 있음.\n",
    "<img src='./img/fig 5-21.png' width=300>\n",
    "\n",
    "\n",
    "- 순전파의 출력 y만으로 계산할 수 있음\n",
    "<img src='./img/e 5.12.png' width=300>\n",
    "\n",
    "<img src='./img/fig 5-22.png' width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기\n",
    "P.170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Affine 계층\n",
    "\n",
    "행렬의 곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2)  # 입력\n",
    "W = np.random.rand(2,3)  # 가중치\n",
    "B = np.random.rand(3)  # 편향\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/fig 5-24.png' width=300>\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$$\n",
    "\n",
    "- $W^T$ : 전치행렬. (i, j) 위치의 원소를 (j, i) 위치로 바꾼 것\n",
    "\n",
    "<img src='./img/e 5.14.png' width=160>\n",
    "\n",
    "<img src='./img/fig 5-25.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 배치용 Affine 계층\n",
    "\n",
    "<img src='./img/fig 5-27.png' width=500>\n",
    "\n",
    "- 편향의 역전파는 데이터에 대한 미분을 데이터마다 더해서 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "<img src='./img/fig 5-28.png' width=600>\n",
    "Softmax 계층은 입력 값을 정규화(출력의 합이 1)하여 출력함.\n",
    "\n",
    "※ NOTE_\n",
    "- 추론 - Softmax 계층을 사용하지 않음\n",
    "- 학습 - Softmax 계층이 필요함\n",
    "\n",
    "<img src='./img/fig 5-29.png' width=600>\n",
    "\n",
    "<img src='./img/fig 5-30.png' width=550>\n",
    "\n",
    "($y_1 - t_1, \\, y_2 - t_2, \\, y_3 - t_3$)\n",
    "\n",
    "역전파의 결과가 Softmax 계층의 출력과 정답 레이블의 차분, 즉 __현재 출력과 정답 레이블의 오차__\n",
    "\n",
    "※ NOTE_\n",
    "\n",
    "우연이 아니라 그렇게 설계되었기 때문\n",
    "- 소프트맥스 함수 - 교차 엔트로피 오차\n",
    "- 항등 함수 - 오차제곱합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None  # 손실\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블(원-핫 벡터)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기\n",
    "P.179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "**전제**<br>\n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 합니다. <br>\n",
    "신경망 학습은 다음과 같이 4단계로 수행합니다.\n",
    "\n",
    "**1단계 - 미니배치**<br>\n",
    "훈련 데이터 중 일부를 무작위로 가져옵니다. <br>\n",
    "이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 중이는 것이 목표입니다.\n",
    "\n",
    "**2단계 - 기울기 산출**<br>\n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다. <br>\n",
    "기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시합니다.\n",
    "\n",
    "**3단계 - 매개변수 갱신**<br>\n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.\n",
    "\n",
    "**4단계 - 반복**<br>\n",
    "1~3단계를 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):  # x : 입력 데이터, t : 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim !=1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):  # x : 입력 데이터, t : 정답 레이블\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.209424773209219e-10\n",
      "b1:2.3861360132687365e-09\n",
      "W2:5.960658212013435e-09\n",
      "b2:1.4045622585279194e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + ':' + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08633333333333333 0.0894\n",
      "0.9031333333333333 0.9081\n",
      "0.9236833333333333 0.9293\n",
      "0.9371 0.9355\n",
      "0.9458166666666666 0.9438\n",
      "0.9497166666666667 0.9464\n",
      "0.9571 0.952\n",
      "0.9605666666666667 0.9559\n",
      "0.9636 0.9579\n",
      "0.9654166666666667 0.9597\n",
      "0.9705 0.9644\n",
      "0.97225 0.9644\n",
      "0.97355 0.9675\n",
      "0.9763 0.967\n",
      "0.9778833333333333 0.9688\n",
      "0.9780666666666666 0.9696\n",
      "0.9794333333333334 0.9705\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기를 구한다.\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
